{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import  DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=200):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], \n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes = 2, vocab_size = 4, embedding_dim = 4, nhead =2, dropout  = 0.2, seq_len = 200 ):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_dim = 64\n",
    "        self.embed = nn.Embedding(vocab_size , embedding_dim)\n",
    "        ######## positioal Embedding ########\n",
    "        self.pe = PositionalEncoding(d_model = embedding_dim, dropout = dropout, max_len = seq_len)\n",
    "        self.ln = nn.Linear(embedding_dim, self.hidden_dim )\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.hidden_dim, nhead = nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        ######## layer CNN ########\n",
    "        self.conv1 = nn.Sequential(\n",
    "                    nn.Conv1d(self.hidden_dim, 128, 5, 1, 2),  \n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "        self.conv2 = nn.Sequential(\n",
    "                    nn.Conv1d(128, 256, 5, 1, 2), \n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "        self.conv3 = nn.Sequential(\n",
    "                    nn.Conv1d(256, 256, 5, 1, 2),  \n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "        ######## layer norm ########\n",
    "        self.normlayer1 = nn.LayerNorm(self.hidden_dim)\n",
    "        self.normlayer2 = nn.LayerNorm(256)\n",
    "        ######## fc ########\n",
    "        self.fc = nn.Linear(256 * seq_len , num_classes)\n",
    "        self.activation = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        ######## Embedding ########\n",
    "        x = self.embed(x)  # batch_size, seq_len, emb_dim =  batch_size*200*2\n",
    "        ######## Pe ########\n",
    "        se = x        \n",
    "        x = self.pe(x)  # batch_size, seq_len, emb_dim\n",
    "        x = x + se\n",
    "        x = self.ln(x)\n",
    "        se = x\n",
    "        x = self.transformer_encoder(x)        \n",
    "        x = x + se\n",
    "        ######## conv and lstm ########\n",
    "        x = x.permute(0,2,1)  # batch_size, hidden_dim, seq_len\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.permute(0,2,1)  # batch_size, hidden_dim, seq_len\n",
    "        ####### layer norm ########\n",
    "        x = self.normlayer2(x)\n",
    "        x = self.normlayer2(x)\n",
    "        ######## fc ########\n",
    "        x =  x.contiguous().view((x.size()[0], -1)) # batch_size, seq_len*hidden_dim*2\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting parameters and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguemnts\n",
    "parser = argparse.ArgumentParser(description='SeqGAN')\n",
    "parser.add_argument('--hpc', action='store_true', default=False,\n",
    "                    help='set to hpc mode')\n",
    "parser.add_argument('--data_path', type=str, default='.seq_gan/', metavar='PATH',\n",
    "                    help='data path to save files (default: /scratch/zc807/seq_gan/)')\n",
    "parser.add_argument('--vocab_size', type=int, default=4, metavar='N',\n",
    "                    help='vocabulary size (default: 10)')\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if not args.hpc:\n",
    "    args.data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab =['Bac', 'Ent', 'Esc','Lis','Pse', 'Sal', 'Sta','Sac','chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY','human','WH','Alpha','Beta','Delta','Gamma','Omicron','yeast',\">1\",\t\">2\",\t\">4\",\t\">3\",\t\">6\",\t\">5\",\t\">X\",\t\">7\",\t\">8\",\t\">12\",\t\">10\",\t\">11\",\t\">9\",\t\">16\",\t\">13\",\t\">17\",\t\">14\",\t\">15\",\t\">18\",\t\">19\",\t\">20\",\t\">21\",\t\">22\",\t\">KI270438.1\",\t\">KI270733.1\",\t\">Y\",\t\">GL000220.1\",\t\">GL000225.1\",\t\">MT\",\t\">GL000224.1\",\t\">KI270709.1\",\t\">KI270728.1\",\t\">KI270729.1\",\t\">KI270736.1\",\t\">GL000216.2\",\t\">GL000218.1\",\t\">KI270442.1\",\t\">KI270467.1\",\t\">GL000195.1\",\t\">GL000205.2\",\t\">GL000221.1\",\t\">GL000226.1\",\t\">KI270333.1\",\t\">KI270538.1\",\t\">KI270589.1\",\t\">KI270715.1\",\t\">KI270720.1\",\t\">KI270722.1\",\t\">KI270723.1\",\t\">KI270735.1\",\t\">KI270749.1\",\t\">KI270750.1\",\t\">KI270754.1\",\t\">Staphylococcus_aureus_chromosome\",\t\">Pseudomonas_aeruginosa_complete_genome\",\t\">Listeria_monocytogenes_complete_genome\",\t\">Enterococcus_faecalis_complete_genome\",\t\">BS.pilon.polished.v3.ST170922\",\t\">Escherichia_coli_chromosome\",\t\">Salmonella_enterica_complete_genome\",\t\">Escherichia_coli_plasmid\",\t\">tig00000001\",\t\">tig00000003\",\t\">tig00000018\",\t\">Staphylococcus_aureus_plasmid1\",\t\">tig00000023\",\t\">tig00000308\",\t\">tig00000036\",\t\">tig00000136\",\t\">tig00000031\",\t\">tig00000051\",\t\">tig00000071\",\t\">tig00000072\",\t\">tig00000104\",\t\">tig00000109\",\t\">tig00000011\",\t\">tig00000055\",\t\">tig00000306\",\t\">tig00000063\",\t\">tig00000069\",\t\">tig00000139\",\t\">tig00000140\",\t\">tig00000006\",\t\">tig00000042\",\t\">tig00000080\",\t\">tig00000094\",\t\">tig00000105\",\t\">tig00000307\",'fly','omicron','phage','sars','ebola','mouse','tick','HAdV']\n",
    "\n",
    "class label2int:\n",
    "    def __init__(self, vocab =vocab):\n",
    "        self.int_map = {}\n",
    "        self.base_map = {}\n",
    "        for ind, base in enumerate(vocab):\n",
    "            self.int_map[base] = ind\n",
    "            self.base_map[ind] = base\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        ch = self.int_map[text]\n",
    "        int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "    \n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.base_map[i])\n",
    "        return ''.join(string)\n",
    "    \n",
    "    \n",
    "convert = label2int()\n",
    "\n",
    "def get_words_from_indices(indices, vocab):\n",
    "    result = [vocab[index] for index in indices]\n",
    "    return result\n",
    "\n",
    "def compare_tensors(t1, t2, t3):\n",
    "    mask = t1 != t2\n",
    "    result = t3[mask]\n",
    "    return result\n",
    "\n",
    "def count_words(words):\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    return word_counts\n",
    "\n",
    "def words_fun(err_labels):\n",
    "    new_lst1  = [element for sublist in err_labels for element in sublist]\n",
    "    err_labels_ls  = [element for sublist in new_lst1 for element in sublist]\n",
    "    err_labels_result = get_words_from_indices(err_labels_ls, vocab)\n",
    "    word_counts = count_words(err_labels_result)\n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def add_first_elements(data, data_list):\n",
    "    first_elements = [row[0] for row in data]\n",
    "    data_list.extend(first_elements)\n",
    "def add_chrom_elements(data, target_list):\n",
    "    chrom_elements = [row[1] for row in data]\n",
    "    target_list.extend(chrom_elements)\n",
    "\n",
    "\n",
    "def read_test_file(data_file):\n",
    "    loadData = np.load(data_file, allow_pickle=True)\n",
    "    data_l = loadData.tolist()\n",
    "    \n",
    "    data_list = []\n",
    "    target_list = []\n",
    "    add_first_elements(data_l, data_list)\n",
    "    add_chrom_elements(data_l, target_list)\n",
    "\n",
    "    lis = []\n",
    "    for line in data_list:\n",
    "        l = [int(s) for s in list(line.strip().split())]\n",
    "        lis.append(l)  \n",
    "    \n",
    "    return lis , target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/public/data1/zhangyx/Projects/Project_seqGAN/data/test_data/human/human_200000_Zymo_200000.npy'\n",
    "num = 200000  # the number of host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "real_data_lis, target_list = read_test_file(test_file)\n",
    "datas = real_data_lis\n",
    "data_label = [convert.text_to_int(i) for i in target_list]\n",
    "targets= [0 for _ in range(num)] + [1 for _ in range(len(datas)-num)]\n",
    "tensor_dat= TensorDataset(torch.tensor(datas), torch.tensor(targets).long() , torch.tensor(data_label) )\n",
    "test_data_loader = DataLoader(dataset=tensor_dat, batch_size =500, shuffle=True)\n",
    "\n",
    "print('Finish load data..')\n",
    "\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "discriminator = Discriminator(num_classes = 2, vocab_size = 4, embedding_dim = 4, nhead =2, dropout  = 0.2, seq_len = seqlen)\n",
    "\n",
    "\n",
    "time1 = time.time()\n",
    "dis_path = '../../model/human_model.pt'\n",
    "dis_dict = torch.load(dis_path, map_location='cpu')# 先加载参数\n",
    "discriminator.load_state_dict(dis_dict,False)  # 再让模型加载参数, 恢复得到模型\n",
    "\n",
    "if args.cuda:\n",
    "    discriminator = discriminator.cuda()\n",
    "    nll_loss = nll_loss.cuda()\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "correct = 0\n",
    "error = 0\n",
    "total_loss = 0.\n",
    "flag = 0\n",
    "neg_flag= 0\n",
    "pos_flag= 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_score = []\n",
    "err_labels =[]\n",
    "err_datas = []\n",
    "pred_lst=[]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target ,data_label in test_data_loader: \n",
    "        data, target, data_label= data.cuda(), target.cuda(), data_label.cuda()\n",
    "\n",
    "        target = target.contiguous().view(-1)\n",
    "        output = discriminator(data)\n",
    "\n",
    "        result= torch.exp(output)\n",
    "        score = output[:,0]\n",
    "        pred =[]\n",
    "        for x in result[:,1]: \n",
    "            if x > 0.1: \n",
    "                pred.append(0) \n",
    "            else: \n",
    "                pred.append(1) \n",
    "        pred = torch.tensor(pred).cuda()\n",
    "\n",
    "        error_label = compare_tensors(pred.data, target.data, data_label.data)\n",
    "        error_label = error_label.tolist()\n",
    "        error_data = compare_tensors(pred.data, target.data, data.data)\n",
    "        error_data = error_data.tolist()\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "        flag+=1\n",
    "\n",
    "        y_true += target.cpu().tolist()\n",
    "        y_pred += pred.cpu().tolist()\n",
    "        y_score += score.cpu().tolist()\n",
    "        err_labels.append(error_label)\n",
    "        err_datas.append(error_data)\n",
    "                    \n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    cm_dict = {'tn': cm[0, 0], 'fp': cm[0, 1],'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    mcc=matthews_corrcoef(y_true, y_pred)\n",
    "    specificity = cm_dict['tn']/(cm_dict['tn']+cm_dict['fp'])\n",
    "    NPV = cm_dict['tn']/(cm_dict['tn']+cm_dict['fn'])\n",
    "\n",
    "    p, r, t = precision_recall_curve(y_true, y_score)\n",
    "    pr_auc = auc(r, p)\n",
    "    # enrich_ratio = (cm_dict['tn']/ (cm_dict['tn'] + cm_dict['fn'])) / (( cm_dict['tn'] + cm_dict['fp'] ) / ( cm_dict['tn'] + cm_dict['tp'] + cm_dict['fn'] + cm_dict['fp']))   ## if you want to cal host enrichment\n",
    "    enrich_ratio = (cm_dict['tp']/ (cm_dict['tp'] + cm_dict['fp'])) / (( cm_dict['tp'] + cm_dict['fn'] ) / ( cm_dict['tp'] + cm_dict['tn'] + cm_dict['fp'] + cm_dict['fn']))    ## pathogen enrichment \n",
    "\n",
    "\n",
    "    print(\"dis eval acc:{:.4f},dis auc: {:.4f}: ,dis pr-auc: {:.4f}: ,  f1 {:.4f}, mcc {:.4f}, recall {:.4f}, precision {:.4f}, specificity: {:.4f} ,NPV,{:.4f} enrich_ratio,{:.2f}\\n\".format(acc,roc_auc , pr_auc, f1, mcc, recall, precision, specificity, NPV, enrich_ratio))\n",
    "    pred_lst.append([ 'human & Zymo', cm_dict['tp'], cm_dict['tn'], cm_dict['fp'], cm_dict['fn'], acc, roc_auc,pr_auc, f1, mcc, recall, precision, specificity, NPV, enrich_ratio])\n",
    "\n",
    "    print('time',time.time()-time1)\n",
    "    print('time per read',(time.time()-time1)/len(targets))\n",
    "\n",
    "    print('time_with_data_process',time.time()-time_start)\n",
    "    print('time per read',(time.time()-time1)/len(targets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame( pred_lst, columns = [ 'epoch','t-target', 't-host', 'f-target', 'f-nontarget', 'acc', 'roc_auc', 'pr-auc', 'f1', 'mcc', 'recall', 'precision', 'specificity', 'NPV','enrich_ratio'] )\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_file = '/public/data1/zhangyx/Projects/Project_seqGAN/data/test_data/human/200kNA12878/NA12878_200000_zymo_2_1.npy'\n",
    "\n",
    "npy_file = '/public/data1/zhangyx/Projects/Project_seqGAN/data/test_data/human/NA12878_200_ebola_200.npy'\n",
    "# npy_file = '/public/data1/zhangyx/Projects/Project_seqGAN/data/test_data/human/hg002_200_ebola_200.npy'\n",
    "num =200\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "real_data_lis, target_list = read_test_file(npy_file)\n",
    "# print('real_data_lis',real_data_lis)\n",
    "datas = real_data_lis\n",
    "print(len(datas))\n",
    "\n",
    "data_label = [convert.text_to_int(i) for i in target_list]\n",
    "\n",
    "\n",
    "# targets= [0 for _ in range(len(datas)-num)] +  [1 for _ in range(num)]\n",
    "# targets= [1 for _ in range(num)] + [0 for _ in range(len(datas)-num)]\n",
    "targets= [0 for _ in range(num)] + [1 for _ in range(len(datas)-num)]\n",
    "\n",
    "\n",
    "# print(f\"物种: {sp}, 标签长度: {len(targets)}\")\n",
    "\n",
    "tensor_dat= TensorDataset(torch.tensor(datas), torch.tensor(targets).long() , torch.tensor(data_label) )\n",
    "test_data_loader = DataLoader(dataset=tensor_dat, batch_size =500, shuffle=True)\n",
    "\n",
    "print('Finish load data..')\n",
    "print('len(test_data_loader)',len(test_data_loader))\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if not args.hpc:\n",
    "    args.data_path = ''\n",
    "nll_loss = nn.NLLLoss()\n",
    "\n",
    "discriminator = Discriminator(num_classes = 2, vocab_size = 4, embedding_dim = 4, nhead =2, dropout  = 0.2, seq_len = seqlen)\n",
    "# discriminator = Discriminator()\n",
    "# \n",
    "# for epoch in range(2,3):\n",
    "for t_i in np.arange(0.1,0.2,0.1):\n",
    "# for t_i in np.arange(0.9,1,0.1):\n",
    "\n",
    "    time1 = time.time()\n",
    "# for t_i in np.arange(0.1,1,0.1):\n",
    "    # epoch =2\n",
    "    dis_path = '../../save/20230924_human_step200_200bp_es10/model_dis10.pt'\n",
    "    # dis_path = '../../save/20240516_human_step7000_200bp_400k_es15_lstm_5_4_32_test_ce/model_dis2.pt'\n",
    "\n",
    "    # dis_path = '/public/data1/zhangyx/Projects/Project_seqGAN/save/20250106_tick_200_200/model_dis100.pt'\n",
    "    # dis_path = '/public/data1/zhangyx/Projects/Project_seqGAN/save/20250106_tick_200_200/model_dis200.pt'\n",
    "    # dis_path = '/public/data1/zhangyx/Projects/Project_seqGAN/save/20250102_mouse/model_dis19.pt'\n",
    "    # dis_path = '../../save/20240516_human_step7000_200bp_400k_es15_lstm_5_4_32_test_ce/model_dis'+str(epoch)+'.pt'\n",
    "\n",
    "\n",
    "    dis_dict = torch.load(dis_path, map_location='cpu')# 先加载参数\n",
    "    discriminator.load_state_dict(dis_dict,False)  # 再让模型加载参数, 恢复得到模型\n",
    "\n",
    "    if args.cuda:\n",
    "        discriminator = discriminator.cuda()\n",
    "        nll_loss = nll_loss.cuda()\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    error = 0\n",
    "    total_loss = 0.\n",
    "    flag = 0\n",
    "    neg_flag= 0\n",
    "    pos_flag= 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "    err_labels =[]\n",
    "    err_datas = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target ,data_label in test_data_loader: \n",
    "            data, target, data_label= data.cuda(), target.cuda(), data_label.cuda()\n",
    "\n",
    "            target = target.contiguous().view(-1)\n",
    "            output = discriminator(data)\n",
    "\n",
    "            # print(torch.exp(output[0]))\n",
    "            result= torch.exp(output)\n",
    "            # result= output\n",
    "            \n",
    "            # pred = probs_to_prediction(result, 0.5)\n",
    "            # pred = output.data.max(1)[1]\n",
    "            score = output[:,0]\n",
    "            # score = output[:,1]\n",
    "            pred =[]\n",
    "            for x in result[:,1]: \n",
    "                if x > t_i: \n",
    "                    # pred.append(1) \n",
    "                    pred.append(0) \n",
    "                else: \n",
    "                    # pred.append(0) \n",
    "                    pred.append(1) \n",
    "            pred = torch.tensor(pred).cuda()\n",
    "\n",
    "            error_label = compare_tensors(pred.data, target.data, data_label.data)\n",
    "            error_label = error_label.tolist()\n",
    "            \n",
    "            error_data = compare_tensors(pred.data, target.data, data.data)\n",
    "            error_data = error_data.tolist()\n",
    "            \n",
    "\n",
    "                \n",
    "\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "            # loss = ce_loss(output, target)\n",
    "            # total_loss += loss.item()\n",
    "            flag+=1\n",
    "\n",
    "            y_true += target.cpu().tolist()\n",
    "            y_pred += pred.cpu().tolist()\n",
    "            y_score += score.cpu().tolist()\n",
    "            err_labels.append(error_label)\n",
    "            err_datas.append(error_data)\n",
    "                        \n",
    "            # print(err_labels)\n",
    "\n",
    "        \n",
    "        cm=confusion_matrix(y_true, y_pred)\n",
    "        cm_dict = {'tn': cm[0, 0], 'fp': cm[0, 1],'fn': cm[1, 0], 'tp': cm[1, 1]}\n",
    "        # print(cm_dict)\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, y_score)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        mcc=matthews_corrcoef(y_true, y_pred)\n",
    "        specificity = cm_dict['tn']/(cm_dict['tn']+cm_dict['fp'])\n",
    "        NPV = cm_dict['tn']/(cm_dict['tn']+cm_dict['fn'])\n",
    "\n",
    "        p, r, t = precision_recall_curve(y_true, y_score)\n",
    "        pr_auc = auc(r, p)\n",
    "        # enrich_ratio = (cm_dict['tn']/ (cm_dict['tn'] + cm_dict['fn'])) / (( cm_dict['tn'] + cm_dict['fp'] ) / ( cm_dict['tn'] + cm_dict['tp'] + cm_dict['fn'] + cm_dict['fp']))\n",
    "        enrich_ratio = (cm_dict['tp']/ (cm_dict['tp'] + cm_dict['fp'])) / (( cm_dict['tp'] + cm_dict['fn'] ) / ( cm_dict['tp'] + cm_dict['tn'] + cm_dict['fp'] + cm_dict['fn']))\n",
    "\n",
    "\n",
    "        print(\"dis eval acc:{:.4f},dis auc: {:.4f}: ,dis pr-auc: {:.4f}: ,  f1 {:.4f}, mcc {:.4f}, recall {:.4f}, precision {:.4f}, specificity: {:.4f} ,NPV,{:.4f} enrich_ratio,{:.2f}\\n\".format(acc,roc_auc , pr_auc, f1, mcc, recall, precision, specificity, NPV, enrich_ratio))\n",
    "        pred_lst.append([ 'ebola', cm_dict['tp'], cm_dict['tn'], cm_dict['fp'], cm_dict['fn'], acc, roc_auc,pr_auc, f1, mcc, recall, precision, specificity, NPV, enrich_ratio])\n",
    "\n",
    "        print('time',time.time()-time1)\n",
    "        print('time per read',(time.time()-time1)/len(targets))\n",
    "\n",
    "        print('time_with_data_process',time.time()-time_start)\n",
    "        print('time per read',(time.time()-time1)/len(targets))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame( pred_lst, columns = [ 'host & target','t-target', 't-host', 'f-target', 'f-nontarget', 'acc', 'roc_auc', 'pr-auc', 'f1', 'mcc', 'recall', 'precision', 'specificity', 'NPV','enrich_ratio'] )\n",
    "pred_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
